{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AdamW, AutoConfig, AutoTokenizer\n",
    "from electra_model import ElectraForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name' : 'sweep',\n",
    "    'method': 'grid',\n",
    "    'metric' : {\n",
    "        'name': 'test_acc',\n",
    "        'goal': 'maximize'   \n",
    "        },\n",
    "    'parameters' : {\n",
    "        'learning_rate': {\n",
    "            'values': [1e-4, 3e-4, 1e-5, 3e-5, 1e-6, 5e-6]\n",
    "            },\n",
    "        'batch_size': {\n",
    "            'values': [256,128,64,32,16]\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "  def __init__(self,\n",
    "               file_path = \"./data/wellness_text_classification.txt\",\n",
    "               num_label = 359,\n",
    "               device = 'cuda',\n",
    "               max_seq_len = 128,\n",
    "               tokenizer = AutoTokenizer.from_pretrained(\"./pretrained_model\")\n",
    "               ):\n",
    "    self.file_path = file_path\n",
    "    self.device = device\n",
    "    self.data = []\n",
    "    self.tokenizer = tokenizer\n",
    "    file = open(self.file_path, 'r')\n",
    "\n",
    "    while True:\n",
    "      line = file.readline()\n",
    "      if not line:\n",
    "        break\n",
    "      datas = line.split(\"    \")\n",
    "      index_of_words = self.tokenizer.encode(datas[0])\n",
    "      token_type_ids = [0] * len(index_of_words)\n",
    "      attention_mask = [1] * len(index_of_words)\n",
    "\n",
    "      padding_length = max_seq_len - len(index_of_words)\n",
    "\n",
    "      index_of_words += [0] * padding_length\n",
    "      token_type_ids += [0] * padding_length\n",
    "      attention_mask += [0] * padding_length\n",
    "\n",
    "      label = int(datas[1][:-1])\n",
    "\n",
    "      data = {\n",
    "              'input_ids': torch.tensor(index_of_words).to(self.device),\n",
    "              'token_type_ids': torch.tensor(token_type_ids).to(self.device),\n",
    "              'attention_mask': torch.tensor(attention_mask).to(self.device),\n",
    "              'labels': torch.tensor(label).to(self.device)\n",
    "             }\n",
    "\n",
    "      self.data.append(data)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  def __getitem__(self,index):\n",
    "    item = self.data[index]\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            #self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs ,model, optimizer, train_loader, test_loader, test_dataset, patience, bs, lr, wandb):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "    wandb.watch(model, loss, log=\"all\", log_freq=10)\n",
    "    losses = []\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True, delta = 0.01, path=f\"./finetuned_model/{bs}with{lr}.pth\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for _, data in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {'input_ids': data['input_ids'],\n",
    "                      'attention_mask': data['attention_mask'],\n",
    "                      'labels': data['labels']\n",
    "                      }\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_losses = []\n",
    "        acc = 0\n",
    "        for data in test_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': data['input_ids'],\n",
    "                          'attention_mask': data['attention_mask'],\n",
    "                          'labels': data['labels']\n",
    "                          }\n",
    "                outputs = model(**inputs)\n",
    "                test_loss = outputs[0]\n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "                logit = outputs[1]\n",
    "                softmax_logit = torch.softmax(logit, dim=-1)\n",
    "                softmax_logit = softmax_logit.squeeze()\n",
    "                max_index = torch.argmax(softmax_logit).item()\n",
    "                acc += (max_index == inputs['labels']).item()\n",
    "        wandb.log({\n",
    "            \"train_loss\": np.mean(losses), \"test_loss\": np.mean(test_losses),\"test_acc\": acc / len(test_dataset) * 100}, step=epoch)\n",
    "        early_stopping(np.mean(test_losses), model)\n",
    "        if early_stopping.early_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrained_model were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ./pretrained_model and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_data = f\"./data/wellness_text_classification_train.txt\"\n",
    "test_data = f\"./data/wellness_text_classification_test.txt\"\n",
    "finetuned_model = f\"./finetuned_model/psychological_counseling_model.pth\"\n",
    "pretrained_model = \"./pretrained_model\"\n",
    "\n",
    "epochs = 200\n",
    "patience = 30\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "train_dataset = TextClassificationDataset(file_path=train_data, tokenizer=tokenizer, device=device)\n",
    "test_dataset = TextClassificationDataset(file_path=test_data, tokenizer=tokenizer, device=device)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "electra_config = AutoConfig.from_pretrained(pretrained_model)\n",
    "model = ElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=pretrained_model,\n",
    "                                                            config=electra_config,\n",
    "                                                            num_labels=359)\n",
    "model.to(device)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(config=None):\n",
    "    wandb.init(config=config)\n",
    "    w_config = wandb.config\n",
    "    batch_size = w_config.batch_size\n",
    "    learning_rate = w_config.learning_rate\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    train(epochs ,model, optimizer, train_loader, test_loader, test_dataset, patience, batch_size, learning_rate, wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: iz8kk105\n",
      "Sweep URL: https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xukz7uzz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtkwk6428\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Dev\\Python\\공모전\\ai\\wandb\\run-20220518_224203-xukz7uzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tkwk6428/sweep/runs/xukz7uzz\" target=\"_blank\">fancy-sweep-1</a></strong> to <a href=\"https://wandb.ai/tkwk6428/sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▂▁▃▄▄▅▆▆▆▆▆▇▇█▇▇▇▇██▇█▇▇██▇▇▇▇▇██▇███</td></tr><tr><td>test_loss</td><td>▆█▄▃▃▂▁▁▂▃▂▂▃▂▃▃▄▄▅▅▆▅▅▆▆▆▆▇▇▇▇▇▇▇▄▅▅</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>47.07113</td></tr><tr><td>test_loss</td><td>4.70054</td></tr><tr><td>train_loss</td><td>1.17689</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fancy-sweep-1</strong>: <a href=\"https://wandb.ai/tkwk6428/sweep/runs/xukz7uzz\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/runs/xukz7uzz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220518_224203-xukz7uzz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 74bgvtxp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Dev\\Python\\공모전\\ai\\wandb\\run-20220519_002736-74bgvtxp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tkwk6428/sweep/runs/74bgvtxp\" target=\"_blank\">floral-sweep-2</a></strong> to <a href=\"https://wandb.ai/tkwk6428/sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▂▁▂▃▃▃██▇▇▆█▆█▄▅▄▁▁▅▃▃▆▅▁▃▁▁▁▂▁▃▄▁▁</td></tr><tr><td>test_loss</td><td>▄▆▆▄▁▄▃▃▅▆▆▅▅▃▄▅▅▅▆▅▆▆▇▆▇█▇▇▆▅▆▆▆▅▆</td></tr><tr><td>train_loss</td><td>▁▁▁▃█▇▆▅▄▄▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>43.72385</td></tr><tr><td>test_loss</td><td>5.2876</td></tr><tr><td>train_loss</td><td>0.13857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">floral-sweep-2</strong>: <a href=\"https://wandb.ai/tkwk6428/sweep/runs/74bgvtxp\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/runs/74bgvtxp</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220519_002736-74bgvtxp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hs5cm792 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Dev\\Python\\공모전\\ai\\wandb\\run-20220519_024639-hs5cm792</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tkwk6428/sweep/runs/hs5cm792\" target=\"_blank\">scarlet-sweep-3</a></strong> to <a href=\"https://wandb.ai/tkwk6428/sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/sweeps/iz8kk105</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▆▁▇▅▅▅▆▄▃▄▆▅▁▆▂▅▂▂▃▄▅▄▄▃▃▁▂█▆▄▅▇▅▆▃▅▄▂▃▄</td></tr><tr><td>test_loss</td><td>█▆▄▆▆▆▅▆▄▅▅▅▇▄▃▁▃▄▂▄▄▆▅▄▇▇▃▃▅▄▄▄▅▃▇▇▇▇▄█</td></tr><tr><td>train_loss</td><td>▁▃▃▄▇█▇▄▃▃▄▄▃▄▇▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>43.93305</td></tr><tr><td>test_loss</td><td>5.48517</td></tr><tr><td>train_loss</td><td>0.09076</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">scarlet-sweep-3</strong>: <a href=\"https://wandb.ai/tkwk6428/sweep/runs/hs5cm792\" target=\"_blank\">https://wandb.ai/tkwk6428/sweep/runs/hs5cm792</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220519_024639-hs5cm792\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"sweep\", entity='tkwk6428')\n",
    "wandb.agent(sweep_id, run_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5cdae407986fbcf9f40eb4f2caf8136385e94546bed8444298080b1cba2358b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
